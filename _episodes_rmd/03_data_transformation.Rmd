---
title: "Data Transformation"
teaching: 30
exercises: 20
questions:
- "How to use dply to transform data in R?"
- "How to assign values to variables and call functions?"
- "What are R's vectors, vector data types, and vectorization?"
- "How to install packages?"
- "How can I get help in R?"
objectives:
- "To understand variables and how to assign to them"
- "To be able to use mathematical and comparison operations"
- "To be able to call functions"
- "To be able to find and use packages"
- "To be able to read R help files for functions and special operators."
- "To be able to seek help from your peers."
keypoints:
- "R has the usual arithmetic operators and mathematical functions."
- "Use `<-` to assign values to variables."
- "Use `ls()` to list the variables in a program."
- "Use `rm()` to delete objects in a program."
- "Use `install.packages()` to install packages (libraries)."
- "Use `library(packagename)`to make a package available for use"
- "Use `help()` to get online help in R."
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("03-")
# Silently load in the data so the rest of the lesson works
d <- read.csv("https://raw.githubusercontent.com/vsbuffalo/bds-files/master/chapter-08-r/Dataset_S1.txt", header=TRUE)
mtfs <- read.delim("https://raw.githubusercontent.com/vsbuffalo/bds-files/master/chapter-08-r/motif_recombrates.txt", header=TRUE)
#colnames(d)[12] <- "percent.GC"
#d$GC.binned <- cut(d$percent.GC, 5)
#d$diversity <- d$Pi/(10*1000)
```

# Data transformation {#transform}

## Introduction

Every data analysis you conduct will likely involve manipulating dataframes at some point. Quickly extracting, 
transforming, and summarizing information from dataframes is an essential R skill. In this part we'll use Hadley 
Wickham’s dplyr package, which consolidates and simplifies many of the common operations we perform on dataframes. 
In addition, much of dplyr key functionality is written in C++ for speed.

```{r, message=FALSE}
if (!require("dplyr")) install.packages("dplyr") # install dplyr if it's not already installed
library(dplyr)
```

> ## Important!
>
> Take careful note of the conflicts message that's printed when you load the tidyverse. It tells you that dplyr overwrites some 
> functions in base R. If you want to use the base version of these functions after loading dplyr, you'll need to use their full 
> names: `stats::filter()` and `stats::lag()`.
>
{: .callout}

## Dataset

We will use the dataset Dataset_S1.txt from the paper "[The Influence of Recombination on Human Genetic Diversity](http://journals.plos.org/plosgenetics/article?id=10.1371/journal.pgen.0020148)", 
which contains estimates of population genetics statistics such 
as nucleotide diversity (e.g., the columns Pi and Theta), recombination (column Recombination), and sequence 
divergence as estimated by percent identity between human and chimpanzee genomes (column Divergence). 
Other columns contain information about the sequencing depth (depth), and GC content 
(percent.GC) for 1kb windows in human chromosome 20. We’ll only work with a few columns in our examples; 
see the description of Dataset_S1.txt in the original paper for more detail.

> ## Miscellaneous Tips
>
>
> * Files can be downloaded directly from the Internet into a local folder of your choice using the `download.file` function.
> The `read.csv` function can then be executed to read the downloaded file from the download location, for example,
> ```{r eval=FALSE, echo=TRUE}
> download.file("https://raw.githubusercontent.com/vsbuffalo/bds-files/master/chapter-08-r/Dataset_S1.txt", destfile = "data/Dataset_S1.txt")
> d <- read.csv("data/Dataset_S1.txt")
> ```
>
> * Alternatively, you can read files directly from the Internet into R by replacing the file paths with a web address in `read.csv`. In doing this no local copy of the csv file is first saved onto your computer. For example,
> ```{r eval=FALSE, echo=TRUE}
> d <- read.csv("https://raw.githubusercontent.com/vsbuffalo/bds-files/master/chapter-08-r/Dataset_S1.txt")
> ```
>
> * Finally, you can read directly from excel spreadsheets without
> converting them to plain text first by using the [readxl](https://cran.r-project.org/web/packages/readxl/index.html) package.
{: .callout}

Note that R’s read.csv() and read.delim() functions have numerous arguments, many of which will need to be adjusted for certain files you’ll come across in bioinformatics. See table below (Table 8-4 of the Buffalo book) for a list of some commonly used arguments, and/or consult help(read.csv) for full documentation.

image: ![Table 8.4](../fig/table-8.4.png)


Because it’s common to work with dataframes with more rows and columns than fit in your screen, `dplyr` uses 
a simple class called `tbl_df` or `tibble` that wraps dataframes so that they don’t fill your screen when you print them 
(similar to using `head()`). Tibbles are data frames, but slightly tweaked to work better in the tidyverse. Let’s convert our `d` dataframe into a `tbl_df` object with the `tbl_df()` function:

```{r}
d_df <- tbl_df(d)
d_df
```

Notice that the tibble only shows the first few rows and all the columns that fit on one screen. (To see the whole dataset, you can run `View(d_df)` which will open the dataset in the RStudio viewer). Also notice the row of three (or four) letter abbreviations under the column names. These describe the type of each variable:

* `int` stands for integers.

* `dbl` stands for doubles, or real numbers.

* `fctr` stands for factors, which R uses to represent categorical variables
  with fixed possible values.
  
There are four other common types of variables that aren't used in this dataset:

* `chr` stands for character vectors, or strings.

* `dttm` stands for date-times (a date + a time).

* `lgl` stands for logical, vectors that contain only `TRUE` or `FALSE`.

* `date` stands for dates.

Also notice, that standard operations on dataframes work with tibbles:

```{r}
colnames(d_df)[12] <- "percent.GC" #rename X.GC columns
d_df$GC.binned <- cut(d_df$percent.GC, 5) # create a new column from existing data
```

### dplyr basics

In this lesson you are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges:

* Pick observations by their values (`filter()`).
* Reorder the rows (`arrange()`).
* Pick variables by their names (`select()`).
* Create new variables with functions of existing variables (`mutate()`).
* Collapse many values down to a single summary (`summarise()`).

None of these functions perform tasks you can’t accomplish with R’s base functions. 
But dplyr’s advantage is in the added consistency, speed, and versatility of its data manipulation interface. 
dplyr’s design drastically simplifies routine data manipulation and analysis tasks, allowing you to more 
easily and effectively explore your data.

dplyr functions be used in conjunction with `group_by()` which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the verbs for a language of data manipulation.

All verbs work similarly: 

1.  The first argument is a data frame.

1.  The subsequent arguments describe what to do with the data frame,
    using the variable names (without quotes).
    
1.  The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let's dive in and see how these verbs work.


## Filter rows with `filter()`

Let's say we used `summary` to get an idea about the total number of SNPs in different windows:

```{r}
summary(d_df$total.SNPs)
```

We see that this varies quite considerably across all windows on chromosome 20 and the data are right-skewed: the third quartile is 12 SNPs, but the maximum is 93 SNPs. Often we want to investigate such outliers more closely. We can use `filter()` to extract the rows of our dataframe based on their values. The first argument is the name of the data frame. The second and 
subsequent arguments are the expressions that filter the data frame.

```{r}
filter(d_df,total.SNPs >= 85)
```

We can build more elaborate queries by using multiple filters. For example, suppose we wanted to see all windows where Pi (nucleotide diversity) is greater than 16 and percent GC is greater than 80. We’d use:

```{r}
filter(d_df, Pi > 16, percent.GC > 80)
```

When you run that line of code, dplyr executes the filtering operation and returns a new data frame. 
Note, **dplyr functions never modify their inputs**, so if you want to save the result, you'll need to use the 
assignment operator, `<-`:

```{r}
search_db <- filter(d_df, Pi > 16, percent.GC > 80)
```

> ## Miscellaneous Tips
>
> R either prints out the results, or saves them to a variable. If you want to do both, you can wrap the assignment in parentheses:
> 
> ```{r}
> (search_db <- filter(d_df, Pi > 16, percent.GC > 80))
> ```
{: .callout}

### Comparisons

To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. R provides the standard suite: `>`, `>=`, `<`, `<=`, `!=` (not equal), and `==` (equal). 

> ## Common pitfalls
>
> When you're starting out with R, the easiest mistake to make is to use `=` instead of `==` when testing for equality. When this > happens you'll get an informative error:
> 
> ```{r, error = TRUE}
> filter(d_df, total.SNPs = 0)
> ```
>
> There's another common problem you might encounter when using `==`: floating point numbers. These results might surprise you!
> 
> ```{r}
> sqrt(2) ^ 2 == 2
> 1/49 * 49 == 1
> ```
> 
> Computers use finite precision arithmetic (they obviously can't store an infinite number of digits!) so remember that every number >  you see is an approximation. Instead of relying on `==`, use `near()`:
> 
> ```{r}
> near(sqrt(2) ^ 2,  2)
> near(1 / 49 * 49, 1)
> ```
>
{: .callout}


### Logical operators

Multiple arguments to `filter()` are combined with "and": every expression must be true in order for a row to be included in the output. For other types of combinations, you'll need to use Boolean operators yourself: `&` is "and", `|` is "or", and `!` is "not". Figure \@ref(fig:bool-ops) shows the complete set of Boolean operations.

```{r bool-ops, echo = FALSE, fig.cap = "Complete set of boolean operations. `x` is the left-hand circle, `y` is the right-hand circle, and the shaded region show which parts each operator selects."}
knitr::include_graphics("diagrams/transform-logical.png")
```

If you are looking for multiple values within a certain column, use the `x %in% y` shortcut. This will select every row where `x` is one of the values in `y`:

```{r, eval = FALSE}
filter(d_df, total.SNPs %in% c(0,2))
```

Notice, how the following search gives some unexpected results:

```{r, eval = FALSE}
filter(d_df, total.SNPs == 0 | 2)
```

Finally, whenever you start using complicated, multipart expressions in `filter()`, consider making them explicit variables instead. That makes it much easier to check your work. We'll learn how to create new variables shortly.


### Missing values

One important feature of R that can make comparison tricky are missing values, or `NA`s ("not availables"). `NA` represents an unknown value so missing values are "contagious": almost any operation involving an unknown value will also be unknown.

```{r}
NA > 5
10 == NA
NA + 10
NA / 2
NA == NA
```

`filter()` only includes rows where the condition is `TRUE`; it excludes both `FALSE` and `NA` values. If you want to preserve missing values, ask for them explicitly:

```{r}
filter(d_df, is.na(Divergence))
```

### Exercises

1.  Find all windows

    1. With the lowest/highest GC content
    1. That have 0 total SNPs
    1. That are incomplete (<1000 bp)
    1. That have the highest divergence with the chimp
    1. That have less than 5% GC difference from the mean

1.  Another useful dplyr filtering helper is `between()`. What does it do?
    Can you use it to simplify the code needed to answer the previous 
    challenges?

## Add new variables with `mutate()`

It's often useful to add new columns that are functions of existing columns (notice, how we used `d_df$GC.binned <- cut(d_df$percent.GC, 5)` above). That's the job of `mutate()`. 

`mutate()` always adds new columns at the end of your dataset. Remember that when you're in RStudio, the easiest way to see all the columns is `View()`.

Here we'll add an additional column that indicates whether a window is in the centromere region. The positions of the chromosome 20 centromere (based on Giemsa banding) are 25,800,000 to 29,700,000 (see this chapter’s README on GitHub to see how these coordinates were found). To append a column called cent that has TRUE/FALSE values indicating whether the current window is fully within a centromeric region we'll use comparison and logical operations:

```{r}
mutate(d_df, cent = start >= 25800000 & end <= 29700000)
View(d_df)
```

> ## Challenge 3
>
> * Why don't we see the column in the dataset? 
>
> > ## Solution
> > As discussed above, dplyr functions never modify their inputs
> {: .solution}
>
> * What standard R command can we use to create a new column in an existing dataframe?
> > ## Solution
> > ```{r}
> > d_df$cent <- d_df$start >= 25800000 & d_df$end <= 29700000
> > ```
> {: .solution}
>
> * How many windows fall into this centromeric region? 
>
> > ## Solution
> >
> > Use `filter()` 
> > ```{r}
> > filter(d_df,cent==TRUE)
> > ```
> > or sum()
> > 
> > ```{r}
> > sum(d_df$cent)
> > ```
> {: .solution}
{: .challenge}

In the dataset we are using, the diversity estimate Pi is measured per sampling window (1kb) and scaled up by 10x (see supplementary Text S1 for more details). It would be useful to have this scaled as per basepair nucleotide diversity (so as to make the scale more intuitive).

```{r}
mutate(d_df, diversity = Pi / (10*1000)) # rescale, removing 10x and making per bp
```

Notice, that we can combine the two operations in a single command:

```{r}
mutate(d_df, 
       diversity = Pi / (10*1000), 
       cent = start >= 25800000 & end <= 29700000
)
```

If you only want to keep the new variables, use `transmute()`:

```{r}
transmute(d_df,
  diversity = Pi / (10*1000), 
  cent = start >= 25800000 & end <= 29700000
)
```

### Useful creation functions {#mutate-funs}

There are many functions for creating new variables that you can use with `mutate()`. The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. There's no way to list every possible function that you might use, but here's a selection of functions that are frequently useful:

*   Arithmetic operators: `+`, `-`, `*`, `/`, `^`. These are all vectorised,
    using the so called "recycling rules". This is most useful when one of the arguments 
    is a single number: `air_time / 60`, `hours * 60 + minute`, etc.
    
    Arithmetic operators are also useful in conjunction with the aggregate
    functions you'll learn about later. For example, `x / sum(x)` calculates 
    the proportion of a total, and `y - mean(y)` computes the difference from 
    the mean.
    
*   Modular arithmetic: `%/%` (integer division) and `%%` (remainder), where
    `x == y * (x %/% y) + (x %% y)`. Modular arithmetic is a handy tool because 
    it allows you to break integers up into pieces.
  
*   Logs: `log()`, `log2()`, `log10()`. Logarithms are an incredibly useful
    transformation for dealing with data that ranges across multiple orders of
    magnitude. They also convert multiplicative relationships to additive.

*   Offsets: `lead()` and `lag()` allow you to refer to leading or lagging 
    values. This allows you to compute running differences (e.g. `x - lag(x)`) 
    or find when values change (`x != lag(x))`. They are most useful in 
    conjunction with `group_by()`, which you'll learn about shortly.
    
    ```{r}
    (x <- 1:10)
    lag(x)
    lead(x)
    ```
  
*   Cumulative and rolling aggregates: R provides functions for running sums,
    products, mins and maxes: `cumsum()`, `cumprod()`, `cummin()`, `cummax()`; 
    and dplyr provides `cummean()` for cumulative means. If you need rolling
    aggregates (i.e. a sum computed over a rolling window), try the RcppRoll
    package.
    
    ```{r}
    x
    cumsum(x)
    cummean(x)
    ```

*   Logical comparisons, `<`, `<=`, `>`, `>=`, `!=`, which you learned about
    earlier. If you're doing a complex sequence of logical operations it's 
    often a good idea to store the interim values in new variables so you can
    check that each step is working as expected.

*   Ranking: there are a number of ranking functions, but you should 
    start with `min_rank()`. It does the most usual type of ranking 
    (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small
    ranks; use `desc(x)` to give the largest values the smallest ranks. 
    
    ```{r}
    y <- c(1, 2, 2, NA, 3, 4)
    min_rank(y)
    min_rank(desc(y))
    ```
    
    If `min_rank()` doesn't do what you need, look at the variants
    `row_number()`, `dense_rank()`, `percent_rank()`, `cume_dist()`,
    `ntile()`.  See their help pages for more details.
    
    ```{r}
    row_number(y)
    dense_rank(y)
    percent_rank(y)
    cume_dist(y)
    ```

### Exercises

```{r, eval = FALSE, echo = FALSE}
flights <- flights %>% mutate(
  dep_time = hour * 60 + minute,
  arr_time = (arr_time %/% 100) * 60 + (arr_time %% 100),
  airtime2 = arr_time - dep_time,
  dep_sched = dep_time + dep_delay
)

ggplot(flights, aes(dep_sched)) + geom_histogram(binwidth = 60)
ggplot(flights, aes(dep_sched %% 60)) + geom_histogram(binwidth = 1)
ggplot(flights, aes(air_time - airtime2)) + geom_histogram()
```

1.  Currently `dep_time` and `sched_dep_time` are convenient to look at, but
    hard to compute with because they're not really continuous numbers. 
    Convert them to a more convenient representation of number of minutes
    since midnight.
    
1.  Compare `air_time` with `arr_time - dep_time`. What do you expect to see?
    What do you see? What do you need to do to fix it?
    
1.  Compare `dep_time`, `sched_dep_time`, and `dep_delay`. How would you
    expect those three numbers to be related?

1.  Find the 10 most delayed flights using a ranking function. How do you want 
    to handle ties? Carefully read the documentation for `min_rank()`.

1.  What does `1:3 + 1:10` return? Why?

1.  What trigonometric functions does R provide?

## Arrange rows with `arrange()`

`arrange()` works similarly to `filter()` except that instead of selecting rows, it changes their order. It takes a data frame and a set of column names (or more complicated expressions) to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:

```{r}
arrange(d_df, cent, percent.GC)
```

Use `desc()` to re-order by a column in descending order:

```{r}
arrange(d_df, desc(cent), percent.GC)
```

Note, that missing values are always sorted at the end!

### Exercises

1.  How could you use `arrange()` to sort all missing values to the start?
    (Hint: use `is.na()`).
    
1.  Sort `flights` to find the most delayed flights. Find the flights that
    left earliest.

1.  Sort `flights` to find the fastest flights.

1.  Which flights travelled the longest? Which travelled the shortest?

## Select columns with `select()`

It's not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you're actually interested in. `select()` allows you to rapidly zoom in on a useful subset using operations based on the names of the variables.

`select()` is not terribly useful with our data because they only have a few variables, but you can still get the general idea:

```{r}
# Select columns by name
select(flights, year, month, day)
# Select all columns between year and day (inclusive)
select(flights, year:day)
# Select all columns except those from year to day (inclusive)
select(flights, -(year:day))
```

There are a number of helper functions you can use within `select()`:

* `starts_with("abc")`: matches names that begin with "abc".

* `ends_with("xyz")`: matches names that end with "xyz".

* `contains("ijk")`: matches names that contain "ijk".

* `matches("(.)\\1")`: selects variables that match a regular expression.
   This one matches any variables that contain repeated characters. You'll 
   learn more about regular expressions in [strings].
   
*  `num_range("x", 1:3)` matches `x1`, `x2` and `x3`.
   
See `?select` for more details.

`select()` can be used to rename variables, but it's rarely useful because it drops all of the variables not explicitly mentioned. Instead, use `rename()`, which is a variant of `select()` that keeps all the variables that aren't explicitly mentioned:

```{r}
rename(d_df, X.GC = percent.GC) #renaming back our variable
```

Another option is to use `select()` in conjunction with the `everything()` helper. This is useful if you have a handful of variables you'd like to move to the start of the data frame.

```{r}
select(cent, everything())
```

### Exercises

1.  Brainstorm as many ways as possible to select `dep_time`, `dep_delay`,
    `arr_time`, and `arr_delay` from `flights`.
    
1.  What happens if you include the name of a variable multiple times in
    a `select()` call?
  
1.  What does the `one_of()` function do? Why might it be helpful in conjunction
    with this vector?
    
    ```{r}
    vars <- c("year", "month", "day", "dep_delay", "arr_delay")
    ```
    
1.  Does the result of running the following code surprise you?  How do the
    select helpers deal with case by default? How can you change that default?

    ```{r, eval = FALSE}
    select(flights, contains("TIME"))
    ```

## Grouped summaries with `summarise()`

The last key verb is `summarise()`. It collapses a data frame to a single row:

```{r}
summarise(d_df, GC = mean(percent.GC, na.rm = TRUE), averageSNPs=mean(total.SNPs, na.rm = TRUE), allSNPs=sum(total.SNPs))
```

`summarise()` is not terribly useful unless we pair it with `group_by()`. This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the dplyr verbs on a grouped data frame they'll be automatically applied "by group". For example, if we applied exactly the same code to a data frame grouped by position related to centromere:

```{r}
by_cent <- group_by(d_df, cent)
summarise(by_cent, GC = mean(percent.GC, na.rm = TRUE), averageSNPs=mean(total.SNPs, na.rm = TRUE), allSNPs=sum(total.SNPs))
```

Subsetting columns can be a useful way to summarize data across two different conditions. 
For example, we might be curious if the average depth in a window (the depth column) 
differs between very high GC content windows (greater than 80%) and all other windows:

```{r}
by_GC <- group_by(d_df, percent.GC >= 80)
summarize(by_GC, depth=mean(depth))
```

This is a fairly large difference, but it’s important to consider how many windows this includes. 
You can count the number of windows (rows) using the `n()` function:

```{r}
by_GC <- group_by(d_df, percent.GC >= 80)
summarize(by_GC, mean_depth=mean(depth), n_rows=n())
```

> ## Challenge 6
>
> As another example, consider looking at Pi by windows that fall in the centromere and those that do not.
> Does the centromer have higher nucleotide diversity than other regions in these data?
>
> > ## Solutions to challenge 6
> >
> > Because d$cent is a logical vector, we can group by it directly:
> > 
> > ```{r}
> > by_cent <- group_by(d_df, cent)
> > summarize(by_cent, nt_diversity=mean(diversity), min=which.min(diversity))
> > ```
> > Indeed, the centromere does appear to have higher nucleotide diversity than other regions in this data. 
> >
> {: .solution}
{: .challenge}


Together `group_by()` and `summarise()` provide one of the tools that you'll use most commonly when working with dplyr: grouped summaries. But before we go any further with this, we need to introduce a powerful new idea: the pipe.

### Combining multiple operations with the pipe

It looks like we have been creating and naming several intermediate files in our anlysis, even though we didn't care about them. Naming things is hard, so this slows down our analysis. 

There's another way to tackle the same problem with the pipe, `%>%`:

```{r}
d %>% 
  tbl_df() %>%
  rename(percent.GC = X.GC) %>%
  group_by(percent.GC >= 80) %>%
  summarize(mean_depth=mean(depth), n_rows=n(), na.rm = TRUE)
```

This focuses on the transformations, not what's being transformed, which makes the code easier to read. 
You can read it as a series of imperative statements: group, then summarise, then filter. 
As suggested by this reading, a good way to pronounce `%>%` when reading code is "then".

Behind the scenes, `x %>% f(y)` turns into `f(x, y)`, and `x %>% f(y) %>% g(z)` turns into 
`g(f(x, y), z)` and so on. You can use the pipe to rewrite multiple operations in a way that 
you can read left-to-right, top-to-bottom. We'll use piping frequently from now on because 
it considerably improves the readability of code, and we'll come back to it in more detail in [pipes].

Working with the pipe is one of the key criteria for belonging to the tidyverse. The only exception is ggplot2: it was written before the pipe was discovered. However, the next iteration of ggplot2, ggvis, will use the pipe. 

> ## RStudio Tip
> A shortcut for %>% is available in the newest RStudio releases under the keybinding 
> CTRL + SHIFT + M (or CMD + SHIFT + M for OSX).  
> You can also review the set of available keybindings when within RStudio with ALT + SHIFT + K.
> ```
{: .callout}


### Missing values

You may have wondered about the `na.rm` argument we used above. What happens if we don't set it?
Let's look at another dataset:

```{r}
(d <- read.delim("https://github.com/EEOB-BioData/BCB546X-Fall2017/raw/master/UNIX_Assignment/snp_position.txt", stringsAsFactors = FALSE, na.strings = c("unknown","multiple")))
View(d)
```

Here is a somewhat meaningless example, but it illustrates the point:

```{r}
d %>% 
  tbl_df() %>%
  group_by(Chromosome) %>% 
  summarise(position = mean(Position))
```

We get a lot of missing values! That's because aggregation functions obey the usual rule of missing values: if there's any missing value in the input, the output will be a missing value. Fortunately, all aggregation functions have an `na.rm` argument which removes the missing values prior to computation:

```{r}
d %>% 
  tbl_df() %>%
  group_by(Chromosome) %>% 
  summarise(position = mean(Position, na.rm = TRUE))
```

We could have also removed all the rows that have uknown position value prior to the analysis:

```{r}
with_position <- d %>% 
  tbl_df() %>%  
  filter(!is.na(Position))

with_position %>% 
  group_by(Chromosome) %>% 
  summarise(first_position = min(Position))
```

### Counts

Whenever you do any aggregation, it's always a good idea to include either a count (`n()`), or a count of non-missing values (`sum(!is.na(x))`). That way you can check that you're not drawing conclusions based on very small amounts of data. For our 
previous example:

```{r}
with_position %>% 
  group_by(Chromosome) %>% 
  summarise(first_position = min(Position), number = n())
```

When looking at this sort of plot, it's often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups. This is what the following code does, as well as showing you a handy pattern for integrating ggplot2 into dplyr flows. It's a bit painful that you have to switch from `%>%` to `+`, but once you get the hang of it, it's quite convenient.

```{r}
delays %>% 
  filter(n > 25) %>% 
  ggplot(mapping = aes(x = n, y = delay)) + 
    geom_point(alpha = 1/10)
```


### Useful summary functions {#summarise-funs}

Just using means, counts, and sum can get you a long way, but R provides many other useful summary functions:

*   Measures of location: we've used `mean(x)`, but `median(x)` is also
    useful. 
*   Measures of spread: `sd(x)`, `IQR(x)`, `mad(x)`. The mean squared deviation,
    or standard deviation or sd for short, is the standard measure of spread.
    The interquartile range `IQR()` and median absolute deviation `mad(x)`
    are robust equivalents that may be more useful if you have outliers.
*   Measures of rank: `min(x)`, `quantile(x, 0.25)`, `max(x)`. Quantiles
    are a generalisation of the median. For example, `quantile(x, 0.25)`
    will find a value of `x` that is greater than 25% of the values,
    and less than the remaining 75%.
*   Measures of position: `first(x)`, `nth(x, 2)`, `last(x)`. These work 
    similarly to `x[1]`, `x[2]`, and `x[length(x)]` but let you set a default 
    value if that position does not exist (i.e. you're trying to get the 3rd
    element from a group that only has two elements). For example, we can
    find the first and last departure for each day:
*   Counts: You've seen `n()`, which takes no arguments, and returns the 
    size of the current group. To count the number of non-missing values, use
    `sum(!is.na(x))`. To count the number of distinct (unique) values, use
    `n_distinct(x)`.
    
    ```{r}
    with_position %>% 
      count(Chromosome)
    ```
    
*   Counts and proportions of logical values: `sum(x > 10)`, `mean(y == 0)`.
    When used with numeric functions, `TRUE` is converted to 1 and `FALSE` to 0. 
    This makes `sum()` and `mean()` very useful: `sum(x)` gives the number of 
    `TRUE`s in `x`, and `mean(x)` gives the proportion.

### Grouping by multiple variables

When you group by multiple variables, each summary peels off one level of the grouping. That makes it easy to progressively roll up a dataset:

```{r}
daily <- group_by(flights, year, month, day)
(per_day   <- summarise(daily, flights = n()))
(per_month <- summarise(per_day, flights = sum(flights)))
(per_year  <- summarise(per_month, flights = sum(flights)))
```

Be careful when progressively rolling up summaries: it's OK for sums and counts, but you need to think about weighting means and variances, and it's not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median.

### Ungrouping

If you need to remove grouping, and return to operations on ungrouped data, use `ungroup()`. 

```{r}
daily %>% 
  ungroup() %>%             # no longer grouped by date
  summarise(flights = n())  # all flights
```

### Exercises

1.  Brainstorm at least 5 different ways to assess the typical delay 
    characteristics of a group of flights. Consider the following scenarios:
    
    * A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of 
      the time.
      
    * A flight is always 10 minutes late.

    * A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of 
      the time.
      
    * 99% of the time a flight is on time. 1% of the time it's 2 hours late.
    
    Which is more important: arrival delay or departure delay?

1.  Come up with another approach that will give you the same output as 
    `not_cancelled %>% count(dest)` and 
    `not_cancelled %>% count(tailnum, wt = distance)` (without using 
    `count()`).

1.  Our definition of cancelled flights (`is.na(dep_delay) | is.na(arr_delay)`
    ) is slightly suboptimal. Why? Which is the most important column?

1.  Look at the number of cancelled flights per day. Is there a pattern?
    Is the proportion of cancelled flights related to the average delay?

1.  Which carrier has the worst delays? Challenge: can you disentangle the
    effects of bad airports vs. bad carriers? Why/why not? (Hint: think about
    `flights %>% group_by(carrier, dest) %>% summarise(n())`)

1.  What does the `sort` argument to `count()` do. When might you use it?

## Grouped mutates (and filters)

Grouping is most useful in conjunction with `summarise()`, but you can also do convenient operations with `mutate()` and `filter()`:

*   Find the worst members of each group:

    ```{r}
    flights_sml %>% 
      group_by(year, month, day) %>%
      filter(rank(desc(arr_delay)) < 10)
    ```

*   Find all groups bigger than a threshold:

    ```{r}
    popular_dests <- flights %>% 
      group_by(dest) %>% 
      filter(n() > 365)
    popular_dests
    ```

*   Standardise to compute per group metrics:

    ```{r}
    popular_dests %>% 
      filter(arr_delay > 0) %>% 
      mutate(prop_delay = arr_delay / sum(arr_delay)) %>% 
      select(year:day, dest, arr_delay, prop_delay)
    ```

A grouped filter is a grouped mutate followed by an ungrouped filter. I generally avoid them except for quick and dirty manipulations: otherwise it's hard to check that you've done the manipulation correctly.

Functions that work most naturally in grouped mutates and filters are known as  window functions (vs. the summary functions used for summaries). You can learn more about useful window functions in the corresponding vignette: `vignette("window-functions")`.

### Exercises

1.  Refer back to the lists of useful mutate and filtering functions. 
    Describe how each operation changes when you combine it with grouping.

1.  Which plane (`tailnum`) has the worst on-time record?

1.  What time of day should you fly if you want to avoid delays as much
    as possible?
    
1.  For each destination, compute the total minutes of delay. For each, 
    flight, compute the proportion of the total delay for its destination.
    
1.  Delays are typically temporally correlated: even once the problem that
    caused the initial delay has been resolved, later flights are delayed 
    to allow earlier flights to leave. Using `lag()` explore how the delay
    of a flight is related to the delay of the immediately preceding flight.
    
1.  Look at each destination. Can you find flights that are suspiciously
    fast? (i.e. flights that represent a potential data entry error). Compute
    the air time a flight relative to the shortest flight to that destination.
    Which flights were most delayed in the air?
    
1.  Find all destinations that are flown by at least two carriers. Use that
    information to rank the carriers.

1.  For each plane, count the number of flights before the first delay 
    of greater than 1 hour.
